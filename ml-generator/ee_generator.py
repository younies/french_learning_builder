import json
import os
import argparse
from openai import OpenAI
from typing import List, Dict, Optional, Tuple
import time
import re
from pathlib import Path

class TCFExpressionEcriteGenerator:
    """
    Generator for TCF Canada Expression Ã‰crite practice materials using OpenAI
    """
    
    def __init__(self, api_key: Optional[str] = None, output_base_dir: Optional[str] = None, offline_mode: bool = False):
        """
        Initialize the generator
        
        Args:
            api_key: OpenAI API key (if None, will try to get from environment)
            output_base_dir: Base directory for output files. If None, defaults to
                             a sibling folder '../french_learning/tcf_canada/ee'
                             relative to the project root.
        """
        # Set up OpenAI client (skipped in offline template mode)
        self.offline_mode = offline_mode
        if self.offline_mode:
            self.client = None
        else:
            if api_key:
                self.client = OpenAI(api_key=api_key)
            else:
                api_key = os.getenv('OPENAI_API_KEY')
                if not api_key:
                    raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable or pass api_key parameter, or pass offline_mode=True to generate templates without calling GPT.")
                self.client = OpenAI(api_key=api_key)
        
        # Resolve default output directory to sibling '../french_learning/tcf_canada/ee'
        if output_base_dir is None:
            project_root = Path(__file__).resolve().parent.parent  # .../french_learning_builder
            sibling_root = project_root.parent / 'french_learning' / 'tcf_canada' / 'ee'
            output_base_dir = str(sibling_root)
        
        self.output_base_dir = output_base_dir
        self.task1_dir = os.path.join(output_base_dir, "task1")
        self.task2_dir = os.path.join(output_base_dir, "task2")
        self.task3_dir = os.path.join(output_base_dir, "task3")
        
        # State file to resume progress and numbering across runs
        self.state_file = os.path.join(output_base_dir, ".generation_state_ee.json")
        self.state: Dict[str, int] = {
            "task1_index": 0,        # next topic index to start from (0-based)
            "task2_index": 0,
            "task3_index": 0,
            "task1_sequence": 0,     # next file sequence number base for filenames
            "task2_sequence": 0,
            "task3_sequence": 0
        }
        
        # Load prompts
        self.task1_prompt = self._load_prompt("ml-generator/ee_task1_prompt.txt")
        self.task2_prompt = self._load_prompt("ml-generator/ee_task2_prompt.txt")
        self.task3_prompt = self._load_prompt("ml-generator/ee_task3_prompt.txt")
        
        # Statistics
        self.stats = {
            "task1_generated": 0,
            "task2_generated": 0,
            "task3_generated": 0,
            "total_api_calls": 0,
            "errors": 0,
            "skipped": 0
        }

    def _load_state(self) -> None:
        """Load resume state from disk if available."""
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # Only update known keys to avoid issues
                for k in self.state.keys():
                    if k in data and isinstance(data[k], int) and data[k] >= 0:
                        self.state[k] = data[k]
                print(f"ğŸ§­ Resume state loaded: {self.state}")
            else:
                print("ğŸ§­ No prior state found. Starting from the beginning.")
        except Exception as e:
            print(f"âš ï¸ Could not load state: {e}. Starting fresh.")

    def _save_state(self) -> None:
        """Persist resume state to disk."""
        try:
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ State saved: {self.state}")
        except Exception as e:
            print(f"âš ï¸ Could not save state: {e}")
    
    def _load_prompt(self, prompt_file: str) -> str:
        """Load prompt from file"""
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Prompt file not found: {prompt_file}")
    
    def _create_directories(self) -> None:
        """Create output directories if they don't exist and load state."""
        os.makedirs(self.task1_dir, exist_ok=True)
        os.makedirs(self.task2_dir, exist_ok=True)
        os.makedirs(self.task3_dir, exist_ok=True)
        print(f"ğŸ“ Directories ready:\n   - Task 1: {self.task1_dir}\n   - Task 2: {self.task2_dir}\n   - Task 3: {self.task3_dir}")
        
        # Load prior state if any
        self._load_state()
        
        # Initialize sequence from existing files if larger than state
        try:
            def max_seq_in(dir_path: str, prefix: str) -> int:
                seq = 0
                for name in os.listdir(dir_path):
                    if name.startswith(prefix + "_"):
                        parts = name.split("_")
                        if len(parts) >= 2 and parts[1].isdigit():
                            seq = max(seq, int(parts[1]))
                return seq
            existing_t1 = max_seq_in(self.task1_dir, "task1")
            existing_t2 = max_seq_in(self.task2_dir, "task2")
            existing_t3 = max_seq_in(self.task3_dir, "task3")
            if existing_t1 > self.state["task1_sequence"]:
                self.state["task1_sequence"] = existing_t1
            if existing_t2 > self.state["task2_sequence"]:
                self.state["task2_sequence"] = existing_t2
            if existing_t3 > self.state["task3_sequence"]:
                self.state["task3_sequence"] = existing_t3
        except Exception:
            pass
    
    def _sanitize_filename(self, content: str, max_length: int = 100) -> str:
        """
        Create a safe filename from topic content
        """
        # Take first part of content for filename
        filename = content[:max_length]
        
        # Remove or replace unsafe characters
        filename = re.sub(r'[^\w\s\-.]', '', filename)
        filename = re.sub(r'\s+', '_', filename)
        filename = filename.strip('_')
        
        # Ensure it's not empty
        if not filename:
            filename = "topic"
        
        return filename + ".md"
    
    def _call_openai(self, prompt: str, topic_content: str, max_retries: int = 3) -> Optional[str]:
        """
        Make API call to OpenAI with retry logic using GPT-5 responses.create format
        """
        full_prompt = f"{prompt}\n\n---\n\n**TOPIC:** {topic_content}"
        
        for attempt in range(max_retries):
            try:
                # Using GPT-4 with standard chat completions API (reliable and proven)
                result = self.client.chat.completions.create(
                    model="gpt-4",
                    messages=[
                        {"role": "system", "content": "You are a French language expert and TCF Canada examiner."},
                        {"role": "user", "content": full_prompt}
                    ],
                    max_tokens=4000,
                    temperature=0.7
                )
                
                self.stats["total_api_calls"] += 1
                
                # Handle standard GPT-4 chat completions response format
                return result.choices[0].message.content.strip()
                
            except Exception as e:
                error_str = str(e).lower()
                
                # Handle rate limit errors
                if "rate limit" in error_str or "quota" in error_str:
                    wait_time = 2 ** attempt  # Exponential backoff
                    print(f"   â³ Rate limit hit, waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    continue
                
                # Handle API errors
                elif "api" in error_str or "request" in error_str:
                    print(f"   âŒ API Error (attempt {attempt + 1}): {e}")
                    if attempt == max_retries - 1:
                        self.stats["errors"] += 1
                        return None
                    time.sleep(1)
                    continue
                
                # Handle other errors
                else:
                    print(f"   âŒ Unexpected error (attempt {attempt + 1}): {e}")
                    if attempt == max_retries - 1:
                        self.stats["errors"] += 1
                        return None
                    time.sleep(1)
                    continue
        
        return None
    
    def _save_markdown_file(self, content: str, filepath: str, topic_info: Dict) -> bool:
        """
        Save generated content to markdown file with metadata
        """
        try:
            task_key = topic_info.get('task', 'unknown')
            task_labels = {
                'tache_1': 'TÃ¢che 1 - Message Personnel',
                'tache_2': 'TÃ¢che 2 - Article/Blog', 
                'tache_3': 'TÃ¢che 3 - Texte Argumentatif'
            }
            task_label = task_labels.get(task_key, str(task_key))
            
            word_count = topic_info.get('word_count', 'Non spÃ©cifiÃ©')
            
            metadata = f"""---
# TCF Canada Expression Ã‰crite - Generated Practice
source_file: {topic_info.get('source_file', 'unknown')}
source_url: {topic_info.get('source_url', 'unknown')}
task: {task_key}
word_count: {word_count}
generated_at: {time.strftime('%Y-%m-%d %H:%M:%S')}
---

# Informations
- TÃ¢che: {task_label}
- Nombre de mots: {word_count}
- Source URL: {topic_info.get('source_url', 'unknown')}
- Source fichier: {topic_info.get('source_file', 'unknown')}

---

# Original Topic
{topic_info.get('content', 'No content available')}

---

"""
            
            full_content = metadata + content
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(full_content)
            
            return True
            
        except Exception as e:
            print(f"   âŒ Error saving file {filepath}: {e}")
            return False

    def _build_template_content(self, task_key: str, topic_info: Dict) -> str:
        """
        Build a Markdown template (no solution) for the given task and topic.
        Includes space/placeholders for the user to write the answer.
        """
        content_lines: List[str] = []

        # Common header
        content_lines.append("# Sujet")
        content_lines.append(topic_info.get('content', ''))
        content_lines.append("")

        # Task-specific scaffolding
        if task_key == 'tache_1':
            content_lines.extend([
                "---",
                "# Consigne",
                "RÃ©digez un message personnel (60â€“120 mots) en respectant la situation et les Ã©lÃ©ments demandÃ©s.",
                "",
                "# Ã‰lÃ©ments Ã  inclure",
                "- Salutation et contexte",
                "- RÃ©ponse aux points essentiels du sujet",
                "- Ton adaptÃ© (familier/standard) et connecteurs",
                "",
                "# Votre rÃ©ponse",
                "(Ã‰crivez votre message ici)"
            ])
        elif task_key == 'tache_2':
            content_lines.extend([
                "---",
                "# Consigne",
                "RÃ©digez un article/texte de blog (120â€“150 mots) en partageant une expÃ©rience et en donnant votre avis.",
                "",
                "# Plan suggÃ©rÃ©",
                "1. Introduction (accroche + thÃ¨me)",
                "2. DÃ©veloppement (faits/arguments)",
                "3. Conclusion (bilan/ouverture)",
                "",
                "# Votre rÃ©ponse",
                "(RÃ©digez votre article ici)"
            ])
        elif task_key == 'tache_3':
            # Include documents when available
            documents: List[str] = topic_info.get('documents', []) or []
            content_lines.append("---")
            content_lines.append("# Documents")
            if documents:
                for i, doc in enumerate(documents, 1):
                    content_lines.append(f"## Document {i}")
                    content_lines.append(doc)
                    content_lines.append("")
            else:
                content_lines.append("(Documents non fournis)")
                content_lines.append("")

            # Required structure for Task 3 per spec
            content_lines.extend([
                "# Structure attendue",
                "1) Paragraphe 1 (40â€“65 mots): synthÃ¨se neutre des deux documents (points essentiels + divergence/convergence)",
                "2) Paragraphe 2: votre position et un premier argument (exemple bref)",
                "3) Paragraphe 3: second argument/nuance (exemple bref)",
                "4) Paragraphe 4: conclusion (ouverture/recommandation)",
                "",
                "# Votre rÃ©ponse",
                "## Paragraphe 1 (synthÃ¨se)",
                "(Ã‰crivez ici ~40â€“65 mots)",
                "",
                "## Paragraphe 2 (opinion + argument 1)",
                "(Ã‰crivez ici)",
                "",
                "## Paragraphe 3 (opinion + argument 2/nuance)",
                "(Ã‰crivez ici)",
                "",
                "## Paragraphe 4 (conclusion)",
                "(Ã‰crivez ici)"
            ])
        else:
            content_lines.append("(Template non dÃ©fini pour cette tÃ¢che)")

        content_lines.append("")
        return "\n".join(content_lines)
    
    def load_organized_ee_topics(self, json_file: str = "organized_ee_topics.json") -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        Load topics from organized_ee_topics.json
        
        Returns:
            (task1_topics, task2_topics, task3_topics)
        """
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            task1_topics = data.get('task1_topics', [])
            task2_topics = data.get('task2_topics', [])
            task3_topics = data.get('task3_topics', [])
            
            print(f"ğŸ“Š Loaded Expression Ã‰crite topics:")
            print(f"   - Task 1: {len(task1_topics)} topics")
            print(f"   - Task 2: {len(task2_topics)} topics")
            print(f"   - Task 3: {len(task3_topics)} topics")
            
            return task1_topics, task2_topics, task3_topics
            
        except FileNotFoundError:
            print(f"âŒ File not found: {json_file}")
            return [], [], []
        except Exception as e:
            print(f"âŒ Error loading topics: {e}")
            return [], [], []
    
    def generate_task1_content(self, topics: List[Dict], limit: Optional[int] = None) -> None:
        """
        Generate Task 1 practice content for given topics, resuming from last index.
        """
        start_idx = self.state.get("task1_index", 0)
        if start_idx >= len(topics):
            print("âœ… All Task 1 topics already processed (or start index beyond list).")
            return
        end_idx = len(topics) if limit is None else min(len(topics), start_idx + max(0, limit))
        work_items = topics[start_idx:end_idx]
        print(f"\nğŸ¯ Generating Task 1 content for topics {start_idx + 1} to {end_idx} (of {len(topics)})...")
        
        for offset, topic in enumerate(work_items):
            i_global = start_idx + offset + 1
            print(f"\nğŸ“ Processing Task 1 topic {i_global}/{len(topics)}")
            print(f"   Content: {topic['content'][:80]}...")
            
            if self.offline_mode:
                generated_content = self._build_template_content('tache_1', topic)
            else:
                generated_content = self._call_openai(self.task1_prompt, topic['content'])
            
            if generated_content:
                # Increment persistent sequence and build filename
                self.state["task1_sequence"] = int(self.state.get("task1_sequence", 0)) + 1
                seq = self.state["task1_sequence"]
                filename = self._sanitize_filename(topic['content'])
                filepath = os.path.join(self.task1_dir, f"task1_{seq:03d}_{filename}")
                
                if self._save_markdown_file(generated_content, filepath, topic):
                    print(f"   âœ… Saved: {os.path.basename(filepath)}")
                    self.stats["task1_generated"] += 1
                else:
                    self.stats["errors"] += 1
            else:
                print(f"   âŒ Failed to generate content")
                self.stats["skipped"] += 1
            
            # Advance index and save state after each item
            self.state["task1_index"] = i_global
            self._save_state()
            time.sleep(0.5)
    
    def generate_task2_content(self, topics: List[Dict], limit: Optional[int] = None) -> None:
        """
        Generate Task 2 practice content for given topics, resuming from last index.
        """
        start_idx = self.state.get("task2_index", 0)
        if start_idx >= len(topics):
            print("âœ… All Task 2 topics already processed (or start index beyond list).")
            return
        end_idx = len(topics) if limit is None else min(len(topics), start_idx + max(0, limit))
        work_items = topics[start_idx:end_idx]
        print(f"\nğŸ¯ Generating Task 2 content for topics {start_idx + 1} to {end_idx} (of {len(topics)})...")
        
        for offset, topic in enumerate(work_items):
            i_global = start_idx + offset + 1
            print(f"\nğŸ“ Processing Task 2 topic {i_global}/{len(topics)}")
            print(f"   Content: {topic['content'][:80]}...")
            
            if self.offline_mode:
                generated_content = self._build_template_content('tache_2', topic)
            else:
                generated_content = self._call_openai(self.task2_prompt, topic['content'])
            
            if generated_content:
                # Increment persistent sequence and build filename
                self.state["task2_sequence"] = int(self.state.get("task2_sequence", 0)) + 1
                seq = self.state["task2_sequence"]
                filename = self._sanitize_filename(topic['content'])
                filepath = os.path.join(self.task2_dir, f"task2_{seq:03d}_{filename}")
                
                if self._save_markdown_file(generated_content, filepath, topic):
                    print(f"   âœ… Saved: {os.path.basename(filepath)}")
                    self.stats["task2_generated"] += 1
                else:
                    self.stats["errors"] += 1
            else:
                print(f"   âŒ Failed to generate content")
                self.stats["skipped"] += 1
            
            # Advance index and save state after each item
            self.state["task2_index"] = i_global
            self._save_state()
            time.sleep(0.5)
    
    def generate_task3_content(self, topics: List[Dict], limit: Optional[int] = None) -> None:
        """
        Generate Task 3 practice content for given topics, resuming from last index.
        """
        start_idx = self.state.get("task3_index", 0)
        if start_idx >= len(topics):
            print("âœ… All Task 3 topics already processed (or start index beyond list).")
            return
        end_idx = len(topics) if limit is None else min(len(topics), start_idx + max(0, limit))
        work_items = topics[start_idx:end_idx]
        print(f"\nğŸ¯ Generating Task 3 content for topics {start_idx + 1} to {end_idx} (of {len(topics)})...")
        
        for offset, topic in enumerate(work_items):
            i_global = start_idx + offset + 1
            print(f"\nğŸ“ Processing Task 3 topic {i_global}/{len(topics)}")
            print(f"   Content: {topic['content'][:80]}...")
            
            # For Task 3, include documents in the prompt
            topic_with_docs = topic['content']
            if 'documents' in topic and topic['documents']:
                topic_with_docs += "\n\nDocuments:\n"
                for i, doc in enumerate(topic['documents'], 1):
                    topic_with_docs += f"Document {i}: {doc}\n"
            
            if self.offline_mode:
                # Keep original topic + documents in template
                topic_for_template = dict(topic)
                topic_for_template['content'] = topic['content']
                generated_content = self._build_template_content('tache_3', topic_for_template)
            else:
                generated_content = self._call_openai(self.task3_prompt, topic_with_docs)
            
            if generated_content:
                # Increment persistent sequence and build filename
                self.state["task3_sequence"] = int(self.state.get("task3_sequence", 0)) + 1
                seq = self.state["task3_sequence"]
                filename = self._sanitize_filename(topic['content'])
                filepath = os.path.join(self.task3_dir, f"task3_{seq:03d}_{filename}")
                
                if self._save_markdown_file(generated_content, filepath, topic):
                    print(f"   âœ… Saved: {os.path.basename(filepath)}")
                    self.stats["task3_generated"] += 1
                else:
                    self.stats["errors"] += 1
            else:
                print(f"   âŒ Failed to generate content")
                self.stats["skipped"] += 1
            
            # Advance index and save state after each item
            self.state["task3_index"] = i_global
            self._save_state()
            time.sleep(0.5)
    
    def generate_all_content(self, task1_limit: Optional[int] = None, task2_limit: Optional[int] = None, task3_limit: Optional[int] = None) -> None:
        """
        Generate all content for all three Expression Ã‰crite tasks.
        In offline_mode, this will generate template files instead of calling GPT.
        """
        print("ğŸš€ Starting TCF Expression Ã‰crite Content Generation...")
        
        # Create directories and load/initialize state
        self._create_directories()
        print(f"ğŸ§­ Resume positions -> Task1 index: {self.state.get('task1_index', 0)}, Task2 index: {self.state.get('task2_index', 0)}, Task3 index: {self.state.get('task3_index', 0)}")
        print(f"ğŸ”¢ File sequences -> Task1: {self.state.get('task1_sequence', 0)}, Task2: {self.state.get('task2_sequence', 0)}, Task3: {self.state.get('task3_sequence', 0)}")
        
        # Load topics
        task1_topics, task2_topics, task3_topics = self.load_organized_ee_topics()
        
        if not task1_topics and not task2_topics and not task3_topics:
            print("âŒ No topics loaded. Exiting.")
            return
        
        # Generate Task 1 content
        if task1_topics and (task1_limit is None or task1_limit > 0):
            self.generate_task1_content(task1_topics, task1_limit)
        
        # Generate Task 2 content
        if task2_topics and (task2_limit is None or task2_limit > 0):
            self.generate_task2_content(task2_topics, task2_limit)
        
        # Generate Task 3 content
        if task3_topics and (task3_limit is None or task3_limit > 0):
            self.generate_task3_content(task3_topics, task3_limit)
        
        # Print final statistics
        self._print_final_stats()
    
    def _print_final_stats(self) -> None:
        """Print generation statistics"""
        print(f"\n{'='*60}")
        print("EXPRESSION Ã‰CRITE GENERATION COMPLETE")
        print(f"{'='*60}")
        print(f"ğŸ“Š Statistics:")
        print(f"   - Task 1 files generated: {self.stats['task1_generated']}")
        print(f"   - Task 2 files generated: {self.stats['task2_generated']}")
        print(f"   - Task 3 files generated: {self.stats['task3_generated']}")
        print(f"   - Total API calls made: {self.stats['total_api_calls']}")
        print(f"   - Errors encountered: {self.stats['errors']}")
        print(f"   - Topics skipped: {self.stats['skipped']}")
        
        total_generated = self.stats['task1_generated'] + self.stats['task2_generated'] + self.stats['task3_generated']
        print(f"   - Total files created: {total_generated}")
        
        print(f"\nğŸ“ Output directories:")
        print(f"   - Task 1: {self.task1_dir}")
        print(f"   - Task 2: {self.task2_dir}")
        print(f"   - Task 3: {self.task3_dir}")
    
    def preview_topics(self, num_samples: int = 3) -> None:
        """
        Preview some topics without generating content
        """
        print("ğŸ‘€ Previewing Expression Ã‰crite topics...")
        
        task1_topics, task2_topics, task3_topics = self.load_organized_ee_topics()
        
        print(f"\nğŸ¯ Task 1 Sample Topics ({len(task1_topics)} total):")
        for i, topic in enumerate(task1_topics[:num_samples], 1):
            print(f"   {i}. {topic['content'][:100]}...")
        
        print(f"\nğŸ¯ Task 2 Sample Topics ({len(task2_topics)} total):")
        for i, topic in enumerate(task2_topics[:num_samples], 1):
            print(f"   {i}. {topic['content'][:100]}...")
        
        print(f"\nğŸ¯ Task 3 Sample Topics ({len(task3_topics)} total):")
        for i, topic in enumerate(task3_topics[:num_samples], 1):
            print(f"   {i}. {topic['content'][:100]}...")


def main():
    """
    Main function with example usage
    """
    print("ğŸ“ TCF Canada Expression Ã‰crite Generator")
    print("=" * 50)

    # CLI args
    parser = argparse.ArgumentParser(description="Generate TCF Expression Ã‰crite content or templates")
    parser.add_argument("--offline", action="store_true", help="Generate templates without calling GPT")
    args, unknown = parser.parse_known_args()

    # Determine offline mode: explicit flag OR missing API key
    env_has_key = bool(os.getenv('OPENAI_API_KEY'))
    offline_mode = bool(args.offline or not env_has_key)

    if offline_mode and not args.offline and not env_has_key:
        print("â„¹ï¸ No OPENAI_API_KEY detected â†’ running in offline template mode.")

    # Initialize generator
    try:
        generator = TCFExpressionEcriteGenerator(offline_mode=offline_mode)
    except ValueError as e:
        # Fallback to offline mode automatically
        print(f"âš ï¸ {e}")
        print("â„¹ï¸ Falling back to offline template mode.")
        generator = TCFExpressionEcriteGenerator(offline_mode=True)
    
    # Preview topics
    generator.preview_topics(5)
    
    # Ask user for limits
    print(f"\nğŸ”§ Configuration:{' (offline templates)' if offline_mode else ''}")
    
    try:
        task1_limit = input("Enter limit for Task 1 topics (press Enter for all): ").strip()
        task1_limit = int(task1_limit) if task1_limit else None
        
        task2_limit = input("Enter limit for Task 2 topics (press Enter for all): ").strip()
        task2_limit = int(task2_limit) if task2_limit else None
        
        task3_limit = input("Enter limit for Task 3 topics (press Enter for all): ").strip()
        task3_limit = int(task3_limit) if task3_limit else None
        
        print(f"\nğŸ“‹ Will generate:")
        print(f"   - Task 1: {task1_limit or 'all'} topics")
        print(f"   - Task 2: {task2_limit or 'all'} topics")
        print(f"   - Task 3: {task3_limit or 'all'} topics")
        
        confirm = input("\nProceed with generation? (y/N): ").strip().lower()
        
        if confirm == 'y':
            # Generate content
            generator.generate_all_content(task1_limit, task2_limit, task3_limit)
        else:
            print("âŒ Generation cancelled.")
            
    except KeyboardInterrupt:
        print("\nâŒ Generation interrupted by user.")
    except Exception as e:
        print(f"âŒ Error during generation: {e}")


if __name__ == "__main__":
    main()
